{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Power10 Scraping Challenge\n\n> If you have not had a chance to check out the [Lazy Guide to Web Scraping](https://github.com/sportsdatasolutions/sport_x_code_eis/blob/master/3.projects/LazyGuides/web_scraping.md) please do so! You can find the code for the guide in this [Deepnote Project](https://deepnote.com/project/0d7f30b4-7eb4-4d7e-b601-28085e59e0d3#%2F2.HTMLTable.ipynb).\n\n#### The challenge is to contribute to this scraper so it scrapes `4 years/seasons` worth of rankings data from the `Men` and `Women's` `60m`, `100m` and `200m` events.\n\n1. ##### Below is a sloution to the Data Handling Power10 Bonus Challenge. ***Note the `helpers.py` module containting the functions used in the loop below to clean each Dataframe***.\n\n2. ##### **Simply** make the script below scrape the same tables (M, W, 60, 100, 200) from the `2020`, `2019`, `2018` and `2017` seasons. Don't be afraid to edit the `helpers.py` module. \n\n3. ##### This web scraper will take a ***while*** to run if scraping from all 4 seasons. To speed up development, have it scrape only what is nessisary to test expected behaviour. You could either lessen the amount of variables to loop through our you could utilise [loop statements](https://www.digitalocean.com/community/tutorials/how-to-use-break-continue-and-pass-statements-when-working-with-loops-in-python-3).\n",
      "metadata": {
        "tags": [],
        "output_cleared": false,
        "cell_id": "00000-d287bcbf-bb12-438d-b5a6-085f503e537d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "output_cleared": true,
        "source_hash": "4beb18c1",
        "execution_millis": 9001,
        "cell_id": "00001-75ecc85f-11a8-4c59-ad87-c87f429a1414",
        "deepnote_to_be_reexecuted": false,
        "execution_start": 1611334922659,
        "deepnote_cell_type": "code"
      },
      "source": "import pandas as pd\nimport helpers\n\ngender = ['M', 'W']\nevents = ['60', '100', '200']\n\nfor sex in gender:\n    for event in events:\n        url = f\"https://www.thepowerof10.info/rankings/rankinglist.aspx?event={event}&agegroup=ALL&sex={sex}&year=2020\"\n\n        df = pd.read_html(url, match='Rank')[1]\n\n        df = helpers.clean_power10_table(df, event, sex)\n\n        helpers.write_power10_table_to_csv(df, event ,sex)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Bonus Task! \n> Add the following to your solution above...\n\n#### Edit your solution so it also adds `two` new columns to your data, one containing the `Athlete Profile URLs` and one containing the `Coach Profile URLs`.\n1. ##### **Start** by inspecting the relevant links you are wanting to scrape (within the HTML Table) e.g. Open your Browser Dev Tools > Click the Inspector Tool > Click on a athlete link on a table > you should see the HTML (anchor tag with `href` link) containing the link we want.\n\n2. ##### **Pandas is limited** in the fact it ***can't*** obtain this additional information **embbeded within the HTML**, for that we'll want to use **BeautifulSoup**.\n\n3. ##### The **example code** is importing `BeautifulSoup4` to obtain all the HTML tables (similar to what pandas does behind the scenes). We use the `find_all` method and reference HTML elements that we want data from e.g. `table`.\n\n4. ##### **Now attempt to use BeautifulSoup to create a list of Athlete and Coach URLs to add to the Dataframes within your loop from your main challenge solution.** Even better, try ***refactor*** (once you have an MVP/Working Solution) it into your `helpers.py` module.\n    \n    **Note**: Notice that the urls are note complete urls, for them to be actual links, you'll have to append `https://www.thepowerof10.info/` to each url you scrape off the tables.\n\n    **Hint**: Scrape and Add the list of athlete urls and coach urls, from the html table, to the the main DataFrame you are constructing before proceeding to the cleaning phase!\n\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00004-aaa66d0e-c21d-4663-820a-8bd975772f44",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-64f58e73-887c-4cd0-bd24-bf9c68968e20",
        "output_cleared": true,
        "source_hash": null,
        "execution_millis": 77,
        "deepnote_to_be_reexecuted": false,
        "execution_start": 1609852750841,
        "deepnote_cell_type": "code"
      },
      "source": "import requests\nfrom bs4 import BeautifulSoup\n\nurl = 'https://www.thepowerof10.info/rankings/rankinglist.aspx?event=100&agegroup=ALL&sex=M&year=2020'\n\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.content, 'lxml')\n\nlen(soup.find_all('table')) # number of tables scraped\n\ntable = soup.find_all('table')[6] # 7th table in the list is the HTML we are after  \n\nathlete_url_list = []\n\n# Loop through all Table Rows (tr) and add links to list\nfor tr in table.find_all('tr'):\n    # Use try except block to avoid terminating loop in case of error/exception (not all rows have links)\n    try:\n        # appends link, href in anchor (a) HTML element within Athlete Name row (('td')[6]) if applicable...\n        athlete_url_list.append(tr.find_all('td')[6].a.get('href'))\n    except:\n        # appends 'N/A' if not applicable...\n        athlete_url_list.append('N/A')\n\nathlete_url_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Bonus Bonus Task!\n> With your solution inculding the first bonus....\n\n#### Edit your solution so it also adds an additional `nation` column so we can differenciate between the `English`, `Welsh`, `Scottish` and `Northern Irish` athletes.\n\n1. ##### Firstly you'll realise there is no way of telling from the existing data who is English, Scottish, Welsh etc.\n\n2. ##### You'll want to pay attention to the Power10 Website, specifically the `Region/Nation` filter.\n\n3. ##### You'll most likely want to `scrape` the Regional/national tables separaely, `merge` together then `sort` out the rankings for each Gender, Event and Season...\n\n    **Note**: A full scrape should take between 6-7mins.",
      "metadata": {
        "tags": [],
        "cell_id": "00004-99e08114-32ed-4827-b3fd-93c13d61b9b5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-fc2acb27-3739-4398-b271-17ab6841d056",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=01834284-5a22-433a-9a28-a120736ff2ec' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "deepnote_notebook_id": "6ab0566c-1220-4ead-8031-d2f5aec5efcf",
    "deepnote_execution_queue": [],
    "deepnote": {}
  }
}